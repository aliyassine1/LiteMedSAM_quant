{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aliyassine1/LiteMedSAM_quant/blob/main/Lite_MedSam_size_reduction_quantization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WniNHcsKPGh",
        "outputId": "b082da3c-b2da-41b5-8250-7c9819ab614c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3JdwP7L0sPR",
        "outputId": "349bc0e0-d826-44ec-ca81-e4ce8834242e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (0.9.12)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.16.0+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.20.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.66.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.23.5)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from os import listdir, makedirs\n",
        "from os.path import join, isfile, basename\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from time import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from segment_anything.modeling import MaskDecoder, PromptEncoder, TwoWayTransformer\n",
        "from tiny_vit_sam import TinyViT\n",
        "from matplotlib import pyplot as plt\n",
        "import cv2\n",
        "import argparse\n",
        "from collections import OrderedDict\n",
        "import pandas as pd\n",
        "\n",
        "# medsamlite\n",
        "class MedSAM_Lite(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            image_encoder,\n",
        "            mask_decoder,\n",
        "            prompt_encoder\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.image_encoder = image_encoder\n",
        "        self.mask_decoder = mask_decoder\n",
        "        self.prompt_encoder = prompt_encoder\n",
        "\n",
        "    def forward(self, image, box_np):\n",
        "        image_embedding = self.image_encoder(image) # (B, 256, 64, 64)\n",
        "        # do not compute gradients for prompt encoder\n",
        "        with torch.no_grad():\n",
        "            box_torch = torch.as_tensor(box_np, dtype=torch.float32, device=image.device)\n",
        "            if len(box_torch.shape) == 2:\n",
        "                box_torch = box_torch[:, None, :] # (B, 1, 4)\n",
        "\n",
        "        sparse_embeddings, dense_embeddings = self.prompt_encoder(\n",
        "            points=None,\n",
        "            boxes=box_np,\n",
        "            masks=None,\n",
        "        )\n",
        "        low_res_masks, iou_predictions = self.mask_decoder(\n",
        "            image_embeddings=image_embedding, # (B, 256, 64, 64)\n",
        "            image_pe=self.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n",
        "            sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n",
        "            dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n",
        "            multimask_output=False,\n",
        "          ) # (B, 1, 256, 256)\n",
        "\n",
        "        return low_res_masks\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def postprocess_masks(self, masks, new_size, original_size):\n",
        "        \"\"\"\n",
        "        Do cropping and resizing\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        masks : torch.Tensor\n",
        "            masks predicted by the model\n",
        "        new_size : tuple\n",
        "            the shape of the image after resizing to the longest side of 256\n",
        "        original_size : tuple\n",
        "            the original shape of the image\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            the upsampled mask to the original size\n",
        "        \"\"\"\n",
        "        # Crop\n",
        "        masks = masks[..., :new_size[0], :new_size[1]]\n",
        "        # Resize\n",
        "        masks = F.interpolate(\n",
        "            masks,\n",
        "            size=(original_size[0], original_size[1]),\n",
        "            mode=\"bilinear\",\n",
        "            align_corners=False,\n",
        "        )\n",
        "\n",
        "        return masks\n"
      ],
      "metadata": {
        "id": "xZ8exKF-RHXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from skimage import transform, io\n",
        "\n",
        "from torch.nn import functional as F\n",
        "from PIL import Image\n",
        "from segment_anything import sam_model_registry\n",
        "from os import listdir, makedirs\n",
        "from os.path import join, isfile, basename\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "# wrap it up as a function\n",
        "import base64\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from tiny_vit_sam import TinyViT\n",
        "from segment_anything.modeling import MaskDecoder, PromptEncoder, TwoWayTransformer\n",
        "#from tiny_vit_sam import TinyViT\n",
        "from matplotlib import pyplot as plt\n",
        "import cv2\n",
        "import argparse\n",
        "from collections import OrderedDict\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# freeze seeds\n",
        "torch.manual_seed(2023)\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.manual_seed(2023)\n",
        "np.random.seed(2023)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def set_up_model():\n",
        "  # set up model\n",
        "      medsam_lite_image_encoder = TinyViT(\n",
        "          img_size=256,\n",
        "          in_chans=3,\n",
        "          embed_dims=[\n",
        "              64, ## (64, 256, 256)\n",
        "              128, ## (128, 128, 128)\n",
        "              160, ## (160, 64, 64)\n",
        "              320 ## (320, 64, 64)\n",
        "          ],\n",
        "          depths=[2, 2, 6, 2],\n",
        "          num_heads=[2, 4, 5, 10],\n",
        "          window_sizes=[7, 7, 14, 7],\n",
        "          mlp_ratio=4.,\n",
        "          drop_rate=0.,\n",
        "          drop_path_rate=0.0,\n",
        "          use_checkpoint=False,\n",
        "          mbconv_expand_ratio=4.0,\n",
        "          local_conv_size=3,\n",
        "          layer_lr_decay=0.8\n",
        "      )\n",
        "\n",
        "      medsam_lite_prompt_encoder = PromptEncoder(\n",
        "          embed_dim=256,\n",
        "          image_embedding_size=(64, 64),\n",
        "          input_image_size=(256, 256),\n",
        "          mask_in_chans=16\n",
        "      )\n",
        "\n",
        "      medsam_lite_mask_decoder = MaskDecoder(\n",
        "          num_multimask_outputs=3,\n",
        "              transformer=TwoWayTransformer(\n",
        "                  depth=2,\n",
        "                  embedding_dim=256,\n",
        "                  mlp_dim=2048,\n",
        "                  num_heads=8,\n",
        "              ),\n",
        "              transformer_dim=256,\n",
        "              iou_head_depth=3,\n",
        "              iou_head_hidden_dim=256,\n",
        "      )\n",
        "\n",
        "      medsam_model = MedSAM_Lite(\n",
        "          image_encoder = medsam_lite_image_encoder,\n",
        "          mask_decoder = medsam_lite_mask_decoder,\n",
        "          prompt_encoder = medsam_lite_prompt_encoder\n",
        "      )\n",
        "      return medsam_model\n",
        "\n",
        "def postprocess_masks(self, masks, new_size, original_size):\n",
        "    \"\"\"\n",
        "    Do cropping and resizing\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    masks : torch.Tensor\n",
        "        masks predicted by the model\n",
        "    new_size : tuple\n",
        "        the shape of the image after resizing to the longest side of 256\n",
        "    original_size : tuple\n",
        "        the original shape of the image\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "        the upsampled mask to the original size\n",
        "    \"\"\"\n",
        "    # Crop\n",
        "    masks = masks[..., :new_size[0], :new_size[1]]\n",
        "    # Resize\n",
        "    masks = F.interpolate(\n",
        "        masks,\n",
        "        size=(original_size[0], original_size[1]),\n",
        "        mode=\"bilinear\",\n",
        "        align_corners=False,\n",
        "    )\n",
        "\n",
        "    return masks\n",
        "\n",
        "@torch.no_grad()\n",
        "def medsam_inference(medsam_model, img_embed, box_256, new_size, original_size):\n",
        "    box_torch = torch.as_tensor(box_256, dtype=torch.float, device=img_embed.device)\n",
        "    if len(box_torch.shape) == 2:\n",
        "        box_torch = box_torch[:, None, :] # (B, 1, 4)\n",
        "\n",
        "    sparse_embeddings, dense_embeddings = medsam_model.prompt_encoder(\n",
        "        points = None,\n",
        "        boxes = box_torch,\n",
        "        masks = None,\n",
        "    )\n",
        "    low_res_logits, _ = medsam_model.mask_decoder(\n",
        "        image_embeddings=img_embed, # (B, 256, 64, 64)\n",
        "        image_pe=medsam_model.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n",
        "        sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n",
        "        dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n",
        "        multimask_output=False\n",
        "    )\n",
        "\n",
        "    low_res_pred = medsam_model.postprocess_masks(low_res_logits, new_size, original_size)\n",
        "    low_res_pred = torch.sigmoid(low_res_pred)\n",
        "    low_res_pred = low_res_pred.squeeze().cpu().numpy()\n",
        "    medsam_seg = (low_res_pred > 0.5).astype(np.uint8)\n",
        "\n",
        "    return medsam_seg\n",
        "\n",
        "\n",
        "\n",
        "def resize_longest_side(image, target_length=256):\n",
        "    \"\"\"\n",
        "    Resize image to target_length while keeping the aspect ratio\n",
        "    Expects a numpy array with shape HxWxC in uint8 format.\n",
        "    \"\"\"\n",
        "    oldh, oldw = image.shape[0], image.shape[1]\n",
        "    scale = target_length * 1.0 / max(oldh, oldw)\n",
        "    newh, neww = oldh * scale, oldw * scale\n",
        "    neww, newh = int(neww + 0.5), int(newh + 0.5)\n",
        "    target_size = (neww, newh)\n",
        "\n",
        "    return cv2.resize(image, target_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "def pad_image(image, target_size=256):\n",
        "    \"\"\"\n",
        "    Pad image to target_size\n",
        "    Expects a numpy array with shape HxWxC in uint8 format.\n",
        "    \"\"\"\n",
        "    # Pad\n",
        "    h, w = image.shape[0], image.shape[1]\n",
        "    padh = target_size - h\n",
        "    padw = target_size - w\n",
        "    if len(image.shape) == 3: ## Pad image\n",
        "        image_padded = np.pad(image, ((0, padh), (0, padw), (0, 0)))\n",
        "    else: ## Pad gt mask\n",
        "        image_padded = np.pad(image, ((0, padh), (0, padw)))\n",
        "\n",
        "    return image_padded\n",
        "\n",
        "def load_image(img_np,medsam_model):\n",
        "\n",
        "\n",
        "    if len(img_np.shape) == 2:\n",
        "        img_3c = np.repeat(img_np[:, :, None], 3, axis=-1)\n",
        "    else:\n",
        "        img_3c = img_np\n",
        "    assert np.max(img_3c)<256, f'input data should be in range [0, 255], but got {np.unique(img_3c)}'\n",
        "    H, W, _ = img_3c.shape\n",
        "\n",
        "    #segs = np.zeros(img_3c.shape[:2], dtype=np.uint8)\n",
        "\n",
        "    segs = np.zeros(img_3c.shape[:2], dtype=np.uint8)\n",
        "\n",
        "    ## MedSAM Lite preprocessing\n",
        "    img_256 = resize_longest_side(img_3c, 256)\n",
        "    newh, neww = img_256.shape[:2]\n",
        "    img_256_norm = (img_256 - img_256.min()) / np.clip(\n",
        "        img_256.max() - img_256.min(), a_min=1e-8, a_max=None\n",
        "    )\n",
        "    ## preprocessing\n",
        "    #img_256 = resize_longest_side(img_3c, 256)\n",
        "    #newh, neww = img_256.shape[:2]\n",
        "    img_256_padded = pad_image(img_256_norm, 256)\n",
        "    img_256_tensor = torch.tensor(img_256_padded).float().permute(2, 0, 1).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image_embedding = medsam_model.image_encoder(img_256_tensor)\n",
        "\n",
        "    return img_3c,image_embedding,H, W,newh, neww\n",
        "\n",
        "\n",
        "\n",
        "def save_mask(self):\n",
        "    out_path = f\"{self.image_path.split('.')[0]}_mask.png\"\n",
        "    io.imsave(out_path, self.mask_c)"
      ],
      "metadata": {
        "id": "C5nO_FEsgL7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantizing the whole linear layers of the models form 32 to 8 bits."
      ],
      "metadata": {
        "id": "hzI04NhwR7xc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lite_medsam_checkpoint_path=\"/content/drive/MyDrive/challenge_medsam/MedSAM_fast/work_dir/LiteMedSAM/lite_medsam.pth\"\n",
        "medsam_model=set_up_model()\n",
        "print(\"Loading MedSAM model, a sec.\")\n",
        "tic = time.perf_counter()\n",
        "lite_medsam_checkpoint = torch.load(lite_medsam_checkpoint_path, map_location=device)\n",
        "medsam_model.load_state_dict(lite_medsam_checkpoint)\n",
        "medsam_model.to(device)\n",
        "medsam_model.eval\n",
        "\n",
        "print(f\"Done, took {time.perf_counter() - tic} to load the model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8a92aaf-b335-415a-f812-b21e1f5abecd",
        "id": "Lep42FWsSFTm"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading MedSAM model, a sec.\n",
            "Done, took 0.34889948300042306 to load the model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Size of the model before quantization')\n",
        "print_size_of_model(medsam_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFjyUBqmW6zu",
        "outputId": "6bdb4994-f92e-4c04-a7db-95053156b5b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of the model before quantization\n",
            "Size (KB): 39376.561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**size reduction by 65.79 % !**"
      ],
      "metadata": {
        "id": "P7KfZRHVDznX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.ao.quantization import quantize_dynamic\n",
        "\n",
        "# Assuming img_encoder is your model\n",
        "medsam_model.eval()  # Ensure the model is in evaluation mode\n",
        "\n",
        "# Perform dynamic quantization, focusing on supported layers\n",
        "quantized_medsam_model = quantize_dynamic(\n",
        "    model=medsam_model,\n",
        "    dtype=torch.qint8,\n",
        "    qconfig_spec={\n",
        "        torch.nn.Linear,  # Add Linear as it's commonly quantized\n",
        "        # Embedding layers are not explicitly included here due to their special requirements\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"Dynamic quantization complete.\")\n",
        "print('Size of the model after quantization')\n",
        "print_size_of_model(quantized_medsam_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHbhjSY6YSWw",
        "outputId": "af39e2f3-7619-493a-adf0-fcca92a2ce2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dynamic quantization complete.\n",
            "Size of the model after quantization\n",
            "Size (KB): 13469.215\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing"
      ],
      "metadata": {
        "id": "e3Cbaer5O6Q7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "non quantized model"
      ],
      "metadata": {
        "id": "NFK8ZGCxPGcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python CVPR24_LiteMedSAM_infer_8bit_quantized.py.py -i /content/drive/MyDrive/challenge_medsam/test_demo/imgs/ -o /content/drive/MyDrive/challenge_medsam/test_demo/T4_env/Final_quant/Original_cpumap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtmnBqrzm81N",
        "outputId": "d34d51fb-2052-4fa7-ee4e-1f2c4aec856e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0% 0/10 [00:00<?, ?it/s]2DBox_CXR_demo.npz, box: [ 84  55 203 193], predicted iou: 0.9303\n",
            " 10% 1/10 [00:03<00:32,  3.65s/it]2DBox_Dermoscopy_demo.npz, box: [124 244 815 843], predicted iou: 0.959\n",
            " 20% 2/10 [00:09<00:38,  4.87s/it]2DBox_Endoscopy_demo.npz, box: [ 564  207 1879 1080], predicted iou: 0.9652\n",
            " 30% 3/10 [00:14<00:34,  4.86s/it]2DBox_Fundus_demo.npz, box: [1150  978 1696 1516], predicted iou: 0.9566\n",
            " 40% 4/10 [00:21<00:33,  5.66s/it]2DBox_Mammography_demo.npz, box: [ 131 1532  475 1895], predicted iou: 0.8332\n",
            "2DBox_Mammography_demo.npz, box: [ 701 1952  816 2096], predicted iou: 0.8386\n",
            "2DBox_Mammography_demo.npz, box: [ 982 1991 1088 2139], predicted iou: 0.8355\n",
            "2DBox_Mammography_demo.npz, box: [ 431 2075  594 2238], predicted iou: 0.7594\n",
            " 50% 5/10 [00:30<00:34,  6.84s/it]2DBox_Microscope_demo.npz, box: [  0  97  66 236], predicted iou: 0.9024\n",
            "2DBox_Microscope_demo.npz, box: [  0 267  84 386], predicted iou: 0.9121\n",
            "2DBox_Microscope_demo.npz, box: [  0 387  34 522], predicted iou: 0.8256\n",
            "2DBox_Microscope_demo.npz, box: [ 29 407 133 522], predicted iou: 0.9182\n",
            "2DBox_Microscope_demo.npz, box: [ 63   0 223 181], predicted iou: 0.9267\n",
            "2DBox_Microscope_demo.npz, box: [ 89 204 232 325], predicted iou: 0.8996\n",
            "2DBox_Microscope_demo.npz, box: [112 310 244 441], predicted iou: 0.9102\n",
            "2DBox_Microscope_demo.npz, box: [122 456 244 522], predicted iou: 0.8763\n",
            "2DBox_Microscope_demo.npz, box: [247 363 384 487], predicted iou: 0.9319\n",
            "2DBox_Microscope_demo.npz, box: [247 108 404 245], predicted iou: 0.9148\n",
            "2DBox_Microscope_demo.npz, box: [251 498 371 522], predicted iou: 0.7983\n",
            "2DBox_Microscope_demo.npz, box: [268   0 391  95], predicted iou: 0.9314\n",
            "2DBox_Microscope_demo.npz, box: [297 240 425 362], predicted iou: 0.9217\n",
            "2DBox_Microscope_demo.npz, box: [381 293 502 435], predicted iou: 0.9115\n",
            "2DBox_Microscope_demo.npz, box: [418   0 552 129], predicted iou: 0.9329\n",
            "2DBox_Microscope_demo.npz, box: [431 153 565 281], predicted iou: 0.9098\n",
            "2DBox_Microscope_demo.npz, box: [493 299 625 443], predicted iou: 0.8909\n",
            "2DBox_Microscope_demo.npz, box: [564   0 703 142], predicted iou: 0.927\n",
            "2DBox_Microscope_demo.npz, box: [590 190 723 318], predicted iou: 0.9168\n",
            "2DBox_Microscope_demo.npz, box: [608 439 754 522], predicted iou: 0.9139\n",
            "2DBox_Microscope_demo.npz, box: [667 298 775 424], predicted iou: 0.9038\n",
            "2DBox_Microscope_demo.npz, box: [694  82 775 190], predicted iou: 0.8812\n",
            " 60% 6/10 [00:43<00:36,  9.16s/it]2DBox_OCT_demo.npz, box: [333 113 350 180], predicted iou: 0.8031\n",
            "2DBox_OCT_demo.npz, box: [370 110 389 173], predicted iou: 0.8028\n",
            "2DBox_OCT_demo.npz, box: [351 115 370 170], predicted iou: 0.8264\n",
            "2DBox_OCT_demo.npz, box: [275 119 330 216], predicted iou: 0.9126\n",
            " 70% 7/10 [00:50<00:24,  8.32s/it]2DBox_US_demo.npz, box: [265  78 347 196], predicted iou: 0.8367\n",
            " 80% 8/10 [00:54<00:13,  6.91s/it]3DBox_CT_demo.npz infer from middle slice to the z_max\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 1/2 [00:02<00:02,  2.46s/it]\u001b[A\n",
            "100% 2/2 [00:05<00:00,  2.51s/it]\n",
            "3DBox_CT_demo.npz infer from middle slice to the z_min\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100% 1/1 [00:03<00:00,  3.60s/it]\n",
            "3DBox_CT_demo.npz infer from middle slice to the z_max\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 1/4 [00:02<00:08,  2.71s/it]\u001b[A\n",
            " 50% 2/4 [00:05<00:05,  2.70s/it]\u001b[A\n",
            " 75% 3/4 [00:08<00:02,  2.68s/it]\u001b[A\n",
            "100% 4/4 [00:10<00:00,  2.73s/it]\n",
            "3DBox_CT_demo.npz infer from middle slice to the z_min\n",
            "\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 33% 1/3 [00:03<00:06,  3.09s/it]\u001b[A\n",
            " 67% 2/3 [00:05<00:02,  2.79s/it]\u001b[A\n",
            "100% 3/3 [00:08<00:00,  2.77s/it]\n",
            " 90% 9/10 [01:23<00:13, 13.90s/it]3DBox_MR_demo.npz infer from middle slice to the z_max\n",
            "\n",
            "  0% 0/37 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 1/37 [00:02<01:33,  2.60s/it]\u001b[A\n",
            "  5% 2/37 [00:06<01:48,  3.10s/it]\u001b[A\n",
            "  8% 3/37 [00:08<01:38,  2.90s/it]\u001b[A\n",
            " 11% 4/37 [00:11<01:31,  2.77s/it]\u001b[A\n",
            " 14% 5/37 [00:14<01:29,  2.80s/it]\u001b[A\n",
            " 16% 6/37 [00:17<01:28,  2.84s/it]\u001b[A\n",
            " 19% 7/37 [00:20<01:26,  2.89s/it]\u001b[A\n",
            " 22% 8/37 [00:22<01:21,  2.80s/it]\u001b[A\n",
            " 24% 9/37 [00:25<01:17,  2.76s/it]\u001b[A\n",
            " 27% 10/37 [00:28<01:13,  2.74s/it]\u001b[A\n",
            " 30% 11/37 [00:31<01:15,  2.91s/it]\u001b[A\n",
            " 32% 12/37 [00:34<01:11,  2.86s/it]\u001b[A\n",
            " 35% 13/37 [00:36<01:06,  2.77s/it]\u001b[A\n",
            " 38% 14/37 [00:39<01:02,  2.70s/it]\u001b[A\n",
            " 41% 15/37 [00:41<00:59,  2.72s/it]\u001b[A\n",
            " 43% 16/37 [00:45<01:03,  3.02s/it]\u001b[A\n",
            " 46% 17/37 [00:48<00:58,  2.94s/it]\u001b[A\n",
            " 49% 18/37 [00:50<00:53,  2.82s/it]\u001b[A\n",
            " 51% 19/37 [00:53<00:50,  2.79s/it]\u001b[A\n",
            " 54% 20/37 [00:56<00:47,  2.81s/it]\u001b[A\n",
            " 57% 21/37 [00:59<00:46,  2.94s/it]\u001b[A\n",
            " 59% 22/37 [01:02<00:42,  2.83s/it]\u001b[A\n",
            " 62% 23/37 [01:05<00:39,  2.79s/it]\u001b[A\n",
            " 65% 24/37 [01:07<00:34,  2.69s/it]\u001b[A\n",
            " 68% 25/37 [01:10<00:33,  2.82s/it]\u001b[A\n",
            " 70% 26/37 [01:13<00:31,  2.88s/it]\u001b[A\n",
            " 73% 27/37 [01:16<00:28,  2.83s/it]\u001b[A\n",
            " 76% 28/37 [01:18<00:24,  2.76s/it]\u001b[A\n",
            " 78% 29/37 [01:21<00:22,  2.76s/it]\u001b[A\n",
            " 81% 30/37 [01:25<00:20,  2.93s/it]\u001b[A\n",
            " 84% 31/37 [01:27<00:17,  2.83s/it]\u001b[A\n",
            " 86% 32/37 [01:30<00:14,  2.81s/it]\u001b[A\n",
            " 89% 33/37 [01:32<00:10,  2.70s/it]\u001b[A\n",
            " 92% 34/37 [01:35<00:07,  2.65s/it]\u001b[A\n",
            " 95% 35/37 [01:38<00:05,  2.91s/it]\u001b[A\n",
            " 97% 36/37 [01:41<00:02,  2.85s/it]\u001b[A\n",
            "100% 37/37 [01:44<00:00,  2.81s/it]\n",
            "3DBox_MR_demo.npz infer from middle slice to the z_min\n",
            "\n",
            "  0% 0/36 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 1/36 [00:02<01:31,  2.62s/it]\u001b[A\n",
            "  6% 2/36 [00:05<01:35,  2.82s/it]\u001b[A\n",
            "  8% 3/36 [00:08<01:36,  2.91s/it]\u001b[A\n",
            " 11% 4/36 [00:11<01:28,  2.76s/it]\u001b[A\n",
            " 14% 5/36 [00:13<01:23,  2.69s/it]\u001b[A\n",
            " 17% 6/36 [00:16<01:19,  2.65s/it]\u001b[A\n",
            " 19% 7/36 [00:19<01:20,  2.79s/it]\u001b[A\n",
            " 22% 8/36 [00:22<01:20,  2.87s/it]\u001b[A\n",
            " 25% 9/36 [00:25<01:17,  2.86s/it]\u001b[A\n",
            " 28% 10/36 [00:27<01:13,  2.82s/it]\u001b[A\n",
            " 31% 11/36 [00:30<01:11,  2.87s/it]\u001b[A\n",
            " 33% 12/36 [00:34<01:12,  3.01s/it]\u001b[A\n",
            " 36% 13/36 [00:36<01:05,  2.87s/it]\u001b[A\n",
            " 39% 14/36 [00:39<01:01,  2.80s/it]\u001b[A\n",
            " 42% 15/36 [00:42<00:58,  2.78s/it]\u001b[A\n",
            " 44% 16/36 [00:45<00:56,  2.82s/it]\u001b[A\n",
            " 47% 17/36 [00:48<00:56,  2.96s/it]\u001b[A\n",
            " 50% 18/36 [00:50<00:50,  2.83s/it]\u001b[A\n",
            " 53% 19/36 [00:53<00:46,  2.72s/it]\u001b[A\n",
            " 56% 20/36 [00:55<00:42,  2.68s/it]\u001b[A\n",
            " 58% 21/36 [00:59<00:42,  2.84s/it]\u001b[A\n",
            " 61% 22/36 [01:02<00:40,  2.91s/it]\u001b[A\n",
            " 64% 23/36 [01:04<00:37,  2.85s/it]\u001b[A\n",
            " 67% 24/36 [01:07<00:34,  2.84s/it]\u001b[A\n",
            " 69% 25/36 [01:10<00:31,  2.83s/it]\u001b[A\n",
            " 72% 26/36 [01:13<00:29,  2.96s/it]\u001b[A\n",
            " 75% 27/36 [01:16<00:25,  2.84s/it]\u001b[A\n",
            " 78% 28/36 [01:19<00:22,  2.77s/it]\u001b[A\n",
            " 81% 29/36 [01:21<00:19,  2.74s/it]\u001b[A\n",
            " 83% 30/36 [01:24<00:16,  2.79s/it]\u001b[A\n",
            " 86% 31/36 [01:27<00:14,  2.91s/it]\u001b[A\n",
            " 89% 32/36 [01:30<00:11,  2.79s/it]\u001b[A\n",
            " 92% 33/36 [01:32<00:08,  2.76s/it]\u001b[A\n",
            " 94% 34/36 [01:35<00:05,  2.68s/it]\u001b[A\n",
            " 97% 35/36 [01:38<00:02,  2.80s/it]\u001b[A\n",
            "100% 36/36 [01:41<00:00,  2.82s/it]\n",
            "100% 10/10 [04:49<00:00, 28.99s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "quantized model"
      ],
      "metadata": {
        "id": "zJSuAi7HPIjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python CVPR24_LiteMedSAM_infer.py -i /content/drive/MyDrive/challenge_medsam/test_demo/imgs/ -o /content/drive/MyDrive/challenge_medsam/test_demo/Final_quant/Quantization_of_whole_linear"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKx5POXdhoqq",
        "outputId": "6fdd8966-be82-48ea-fb31-29b24c3bc06a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0% 0/10 [00:00<?, ?it/s]2DBox_CXR_demo.npz, box: [ 84  55 203 193], predicted iou: 0.9353\n",
            " 10% 1/10 [00:05<00:46,  5.21s/it]2DBox_Dermoscopy_demo.npz, box: [124 244 815 843], predicted iou: 0.9578\n",
            " 20% 2/10 [00:12<00:52,  6.57s/it]2DBox_Endoscopy_demo.npz, box: [ 564  207 1879 1080], predicted iou: 0.9687\n",
            " 30% 3/10 [00:17<00:39,  5.61s/it]2DBox_Fundus_demo.npz, box: [1150  978 1696 1516], predicted iou: 0.9498\n",
            " 40% 4/10 [00:24<00:36,  6.09s/it]2DBox_Mammography_demo.npz, box: [ 131 1532  475 1895], predicted iou: 0.8508\n",
            "2DBox_Mammography_demo.npz, box: [ 701 1952  816 2096], predicted iou: 0.832\n",
            "2DBox_Mammography_demo.npz, box: [ 982 1991 1088 2139], predicted iou: 0.8436\n",
            "2DBox_Mammography_demo.npz, box: [ 431 2075  594 2238], predicted iou: 0.7804\n",
            " 50% 5/10 [00:33<00:36,  7.21s/it]2DBox_Microscope_demo.npz, box: [  0  97  66 236], predicted iou: 0.9119\n",
            "2DBox_Microscope_demo.npz, box: [  0 267  84 386], predicted iou: 0.9206\n",
            "2DBox_Microscope_demo.npz, box: [  0 387  34 522], predicted iou: 0.8271\n",
            "2DBox_Microscope_demo.npz, box: [ 29 407 133 522], predicted iou: 0.9201\n",
            "2DBox_Microscope_demo.npz, box: [ 63   0 223 181], predicted iou: 0.934\n",
            "2DBox_Microscope_demo.npz, box: [ 89 204 232 325], predicted iou: 0.9084\n",
            "2DBox_Microscope_demo.npz, box: [112 310 244 441], predicted iou: 0.9123\n",
            "2DBox_Microscope_demo.npz, box: [122 456 244 522], predicted iou: 0.8852\n",
            "2DBox_Microscope_demo.npz, box: [247 363 384 487], predicted iou: 0.9317\n",
            "2DBox_Microscope_demo.npz, box: [247 108 404 245], predicted iou: 0.9225\n",
            "2DBox_Microscope_demo.npz, box: [251 498 371 522], predicted iou: 0.7953\n",
            "2DBox_Microscope_demo.npz, box: [268   0 391  95], predicted iou: 0.9313\n",
            "2DBox_Microscope_demo.npz, box: [297 240 425 362], predicted iou: 0.9269\n",
            "2DBox_Microscope_demo.npz, box: [381 293 502 435], predicted iou: 0.9199\n",
            "2DBox_Microscope_demo.npz, box: [418   0 552 129], predicted iou: 0.9345\n",
            "2DBox_Microscope_demo.npz, box: [431 153 565 281], predicted iou: 0.9155\n",
            "2DBox_Microscope_demo.npz, box: [493 299 625 443], predicted iou: 0.8996\n",
            "2DBox_Microscope_demo.npz, box: [564   0 703 142], predicted iou: 0.9305\n",
            "2DBox_Microscope_demo.npz, box: [590 190 723 318], predicted iou: 0.9188\n",
            "2DBox_Microscope_demo.npz, box: [608 439 754 522], predicted iou: 0.9183\n",
            "2DBox_Microscope_demo.npz, box: [667 298 775 424], predicted iou: 0.9153\n",
            "2DBox_Microscope_demo.npz, box: [694  82 775 190], predicted iou: 0.8889\n",
            " 60% 6/10 [00:46<00:36,  9.14s/it]2DBox_OCT_demo.npz, box: [333 113 350 180], predicted iou: 0.819\n",
            "2DBox_OCT_demo.npz, box: [370 110 389 173], predicted iou: 0.8074\n",
            "2DBox_OCT_demo.npz, box: [351 115 370 170], predicted iou: 0.8243\n",
            "2DBox_OCT_demo.npz, box: [275 119 330 216], predicted iou: 0.923\n",
            " 70% 7/10 [00:51<00:24,  8.03s/it]2DBox_US_demo.npz, box: [265  78 347 196], predicted iou: 0.8452\n",
            " 80% 8/10 [00:55<00:13,  6.69s/it]3DBox_CT_demo.npz infer from middle slice to the z_max\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 1/2 [00:02<00:02,  2.89s/it]\u001b[A\n",
            "100% 2/2 [00:06<00:00,  3.07s/it]\n",
            "3DBox_CT_demo.npz infer from middle slice to the z_min\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100% 1/1 [00:02<00:00,  2.69s/it]\n",
            "3DBox_CT_demo.npz infer from middle slice to the z_max\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 25% 1/4 [00:03<00:09,  3.31s/it]\u001b[A\n",
            " 50% 2/4 [00:05<00:05,  2.93s/it]\u001b[A\n",
            " 75% 3/4 [00:09<00:03,  3.12s/it]\u001b[A\n",
            "100% 4/4 [00:12<00:00,  3.01s/it]\n",
            "3DBox_CT_demo.npz infer from middle slice to the z_min\n",
            "\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 33% 1/3 [00:02<00:05,  2.67s/it]\u001b[A\n",
            " 67% 2/3 [00:05<00:02,  2.63s/it]\u001b[A\n",
            "100% 3/3 [00:08<00:00,  2.81s/it]\n",
            " 90% 9/10 [01:28<00:14, 14.78s/it]3DBox_MR_demo.npz infer from middle slice to the z_max\n",
            "\n",
            "  0% 0/37 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 1/37 [00:03<02:10,  3.61s/it]\u001b[A\n",
            "  5% 2/37 [00:05<01:39,  2.85s/it]\u001b[A\n",
            "  8% 3/37 [00:08<01:26,  2.56s/it]\u001b[A\n",
            " 11% 4/37 [00:10<01:23,  2.52s/it]\u001b[A\n",
            " 14% 5/37 [00:13<01:25,  2.66s/it]\u001b[A\n",
            " 16% 6/37 [00:15<01:17,  2.50s/it]\u001b[A\n",
            " 19% 7/37 [00:18<01:13,  2.44s/it]\u001b[A\n",
            " 22% 8/37 [00:20<01:09,  2.40s/it]\u001b[A\n",
            " 24% 9/37 [00:22<01:06,  2.38s/it]\u001b[A\n",
            " 27% 10/37 [00:25<01:07,  2.52s/it]\u001b[A\n",
            " 30% 11/37 [00:28<01:05,  2.53s/it]\u001b[A\n",
            " 32% 12/37 [00:30<01:00,  2.42s/it]\u001b[A\n",
            " 35% 13/37 [00:32<00:55,  2.30s/it]\u001b[A\n",
            " 38% 14/37 [00:34<00:52,  2.29s/it]\u001b[A\n",
            " 41% 15/37 [00:37<00:52,  2.37s/it]\u001b[A\n",
            " 43% 16/37 [00:41<01:01,  2.94s/it]\u001b[A\n",
            " 46% 17/37 [00:43<00:55,  2.79s/it]\u001b[A\n",
            " 49% 18/37 [00:46<00:50,  2.63s/it]\u001b[A\n",
            " 51% 19/37 [00:48<00:45,  2.55s/it]\u001b[A\n",
            " 54% 20/37 [00:50<00:40,  2.41s/it]\u001b[A\n",
            " 57% 21/37 [00:53<00:39,  2.47s/it]\u001b[A\n",
            " 59% 22/37 [00:55<00:36,  2.45s/it]\u001b[A\n",
            " 62% 23/37 [00:57<00:34,  2.44s/it]\u001b[A\n",
            " 65% 24/37 [01:00<00:31,  2.44s/it]\u001b[A\n",
            " 68% 25/37 [01:03<00:30,  2.53s/it]\u001b[A\n",
            " 70% 26/37 [01:05<00:28,  2.62s/it]\u001b[A\n",
            " 73% 27/37 [01:08<00:26,  2.67s/it]\u001b[A\n",
            " 76% 28/37 [01:10<00:22,  2.50s/it]\u001b[A\n",
            " 78% 29/37 [01:13<00:19,  2.41s/it]\u001b[A\n",
            " 81% 30/37 [01:15<00:16,  2.37s/it]\u001b[A\n",
            " 84% 31/37 [01:20<00:18,  3.15s/it]\u001b[A\n",
            " 86% 32/37 [01:23<00:16,  3.27s/it]\u001b[A\n",
            " 89% 33/37 [01:26<00:11,  2.96s/it]\u001b[A\n",
            " 92% 34/37 [01:28<00:08,  2.68s/it]\u001b[A\n",
            " 95% 35/37 [01:30<00:05,  2.56s/it]\u001b[A\n",
            " 97% 36/37 [01:32<00:02,  2.54s/it]\u001b[A\n",
            "100% 37/37 [01:35<00:00,  2.59s/it]\n",
            "3DBox_MR_demo.npz infer from middle slice to the z_min\n",
            "\n",
            "  0% 0/36 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 1/36 [00:02<01:12,  2.08s/it]\u001b[A\n",
            "  6% 2/36 [00:04<01:24,  2.50s/it]\u001b[A\n",
            "  8% 3/36 [00:07<01:17,  2.35s/it]\u001b[A\n",
            " 11% 4/36 [00:09<01:12,  2.28s/it]\u001b[A\n",
            " 14% 5/36 [00:13<01:28,  2.85s/it]\u001b[A\n",
            " 17% 6/36 [00:15<01:18,  2.61s/it]\u001b[A\n",
            " 19% 7/36 [00:17<01:12,  2.50s/it]\u001b[A\n",
            " 22% 8/36 [00:19<01:08,  2.43s/it]\u001b[A\n",
            " 25% 9/36 [00:21<01:02,  2.33s/it]\u001b[A\n",
            " 28% 10/36 [00:24<01:01,  2.38s/it]\u001b[A\n",
            " 31% 11/36 [00:26<01:01,  2.44s/it]\u001b[A\n",
            " 33% 12/36 [00:29<00:57,  2.39s/it]\u001b[A\n",
            " 36% 13/36 [00:31<00:55,  2.39s/it]\u001b[A\n",
            " 39% 14/36 [00:33<00:52,  2.36s/it]\u001b[A\n",
            " 42% 15/36 [00:36<00:48,  2.32s/it]\u001b[A\n",
            " 44% 16/36 [00:39<00:50,  2.54s/it]\u001b[A\n",
            " 47% 17/36 [00:41<00:46,  2.46s/it]\u001b[A\n",
            " 50% 18/36 [00:43<00:43,  2.39s/it]\u001b[A\n",
            " 53% 19/36 [00:46<00:43,  2.59s/it]\u001b[A\n",
            " 56% 20/36 [00:49<00:42,  2.67s/it]\u001b[A\n",
            " 58% 21/36 [00:55<00:53,  3.57s/it]\u001b[A\n",
            " 61% 22/36 [00:58<00:50,  3.60s/it]\u001b[A\n",
            " 64% 23/36 [01:01<00:41,  3.16s/it]\u001b[A\n",
            " 67% 24/36 [01:04<00:39,  3.29s/it]\u001b[A\n",
            " 69% 25/36 [01:09<00:42,  3.85s/it]\u001b[A\n",
            " 72% 26/36 [01:12<00:36,  3.62s/it]\u001b[A\n",
            " 75% 27/36 [01:16<00:31,  3.50s/it]\u001b[A\n",
            " 78% 28/36 [01:21<00:32,  4.07s/it]\u001b[A\n",
            " 81% 29/36 [01:23<00:24,  3.52s/it]\u001b[A\n",
            " 83% 30/36 [01:25<00:18,  3.10s/it]\u001b[A\n",
            " 86% 31/36 [01:28<00:15,  3.07s/it]\u001b[A\n",
            " 89% 32/36 [01:32<00:13,  3.28s/it]\u001b[A\n",
            " 92% 33/36 [01:36<00:09,  3.29s/it]\u001b[A\n",
            " 94% 34/36 [01:38<00:05,  2.98s/it]\u001b[A\n",
            " 97% 35/36 [01:40<00:02,  2.76s/it]\u001b[A\n",
            "100% 36/36 [01:42<00:00,  2.86s/it]\n",
            "100% 10/10 [04:47<00:00, 28.76s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both have similar iou pred but the quantized is faster"
      ],
      "metadata": {
        "id": "LAxO-nR7QO2k"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO76pj0CVKblAk2Mz6Keip/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}