{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aliyassine1/LiteMedSAM_quant/blob/main/Lite_MedSam_size_reduction_quantization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WniNHcsKPGh",
        "outputId": "b082da3c-b2da-41b5-8250-7c9819ab614c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3JdwP7L0sPR",
        "outputId": "349bc0e0-d826-44ec-ca81-e4ce8834242e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (0.9.12)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.16.0+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.20.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.66.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.23.5)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from os import listdir, makedirs\n",
        "from os.path import join, isfile, basename\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from time import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from segment_anything.modeling import MaskDecoder, PromptEncoder, TwoWayTransformer\n",
        "from tiny_vit_sam import TinyViT\n",
        "from matplotlib import pyplot as plt\n",
        "import cv2\n",
        "import argparse\n",
        "from collections import OrderedDict\n",
        "import pandas as pd\n",
        "\n",
        "# medsamlite\n",
        "class MedSAM_Lite(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            image_encoder,\n",
        "            mask_decoder,\n",
        "            prompt_encoder\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.image_encoder = image_encoder\n",
        "        self.mask_decoder = mask_decoder\n",
        "        self.prompt_encoder = prompt_encoder\n",
        "\n",
        "    def forward(self, image, box_np):\n",
        "        image_embedding = self.image_encoder(image) # (B, 256, 64, 64)\n",
        "        # do not compute gradients for prompt encoder\n",
        "        with torch.no_grad():\n",
        "            box_torch = torch.as_tensor(box_np, dtype=torch.float32, device=image.device)\n",
        "            if len(box_torch.shape) == 2:\n",
        "                box_torch = box_torch[:, None, :] # (B, 1, 4)\n",
        "\n",
        "        sparse_embeddings, dense_embeddings = self.prompt_encoder(\n",
        "            points=None,\n",
        "            boxes=box_np,\n",
        "            masks=None,\n",
        "        )\n",
        "        low_res_masks, iou_predictions = self.mask_decoder(\n",
        "            image_embeddings=image_embedding, # (B, 256, 64, 64)\n",
        "            image_pe=self.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n",
        "            sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n",
        "            dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n",
        "            multimask_output=False,\n",
        "          ) # (B, 1, 256, 256)\n",
        "\n",
        "        return low_res_masks\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def postprocess_masks(self, masks, new_size, original_size):\n",
        "        \"\"\"\n",
        "        Do cropping and resizing\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        masks : torch.Tensor\n",
        "            masks predicted by the model\n",
        "        new_size : tuple\n",
        "            the shape of the image after resizing to the longest side of 256\n",
        "        original_size : tuple\n",
        "            the original shape of the image\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            the upsampled mask to the original size\n",
        "        \"\"\"\n",
        "        # Crop\n",
        "        masks = masks[..., :new_size[0], :new_size[1]]\n",
        "        # Resize\n",
        "        masks = F.interpolate(\n",
        "            masks,\n",
        "            size=(original_size[0], original_size[1]),\n",
        "            mode=\"bilinear\",\n",
        "            align_corners=False,\n",
        "        )\n",
        "\n",
        "        return masks\n"
      ],
      "metadata": {
        "id": "xZ8exKF-RHXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from skimage import transform, io\n",
        "\n",
        "from torch.nn import functional as F\n",
        "from PIL import Image\n",
        "from segment_anything import sam_model_registry\n",
        "from os import listdir, makedirs\n",
        "from os.path import join, isfile, basename\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "# wrap it up as a function\n",
        "import base64\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from tiny_vit_sam import TinyViT\n",
        "from segment_anything.modeling import MaskDecoder, PromptEncoder, TwoWayTransformer\n",
        "#from tiny_vit_sam import TinyViT\n",
        "from matplotlib import pyplot as plt\n",
        "import cv2\n",
        "import argparse\n",
        "from collections import OrderedDict\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# freeze seeds\n",
        "torch.manual_seed(2023)\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.manual_seed(2023)\n",
        "np.random.seed(2023)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def set_up_model():\n",
        "  # set up model\n",
        "      medsam_lite_image_encoder = TinyViT(\n",
        "          img_size=256,\n",
        "          in_chans=3,\n",
        "          embed_dims=[\n",
        "              64, ## (64, 256, 256)\n",
        "              128, ## (128, 128, 128)\n",
        "              160, ## (160, 64, 64)\n",
        "              320 ## (320, 64, 64)\n",
        "          ],\n",
        "          depths=[2, 2, 6, 2],\n",
        "          num_heads=[2, 4, 5, 10],\n",
        "          window_sizes=[7, 7, 14, 7],\n",
        "          mlp_ratio=4.,\n",
        "          drop_rate=0.,\n",
        "          drop_path_rate=0.0,\n",
        "          use_checkpoint=False,\n",
        "          mbconv_expand_ratio=4.0,\n",
        "          local_conv_size=3,\n",
        "          layer_lr_decay=0.8\n",
        "      )\n",
        "\n",
        "      medsam_lite_prompt_encoder = PromptEncoder(\n",
        "          embed_dim=256,\n",
        "          image_embedding_size=(64, 64),\n",
        "          input_image_size=(256, 256),\n",
        "          mask_in_chans=16\n",
        "      )\n",
        "\n",
        "      medsam_lite_mask_decoder = MaskDecoder(\n",
        "          num_multimask_outputs=3,\n",
        "              transformer=TwoWayTransformer(\n",
        "                  depth=2,\n",
        "                  embedding_dim=256,\n",
        "                  mlp_dim=2048,\n",
        "                  num_heads=8,\n",
        "              ),\n",
        "              transformer_dim=256,\n",
        "              iou_head_depth=3,\n",
        "              iou_head_hidden_dim=256,\n",
        "      )\n",
        "\n",
        "      medsam_model = MedSAM_Lite(\n",
        "          image_encoder = medsam_lite_image_encoder,\n",
        "          mask_decoder = medsam_lite_mask_decoder,\n",
        "          prompt_encoder = medsam_lite_prompt_encoder\n",
        "      )\n",
        "      return medsam_model\n",
        "\n",
        "def postprocess_masks(self, masks, new_size, original_size):\n",
        "    \"\"\"\n",
        "    Do cropping and resizing\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    masks : torch.Tensor\n",
        "        masks predicted by the model\n",
        "    new_size : tuple\n",
        "        the shape of the image after resizing to the longest side of 256\n",
        "    original_size : tuple\n",
        "        the original shape of the image\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "        the upsampled mask to the original size\n",
        "    \"\"\"\n",
        "    # Crop\n",
        "    masks = masks[..., :new_size[0], :new_size[1]]\n",
        "    # Resize\n",
        "    masks = F.interpolate(\n",
        "        masks,\n",
        "        size=(original_size[0], original_size[1]),\n",
        "        mode=\"bilinear\",\n",
        "        align_corners=False,\n",
        "    )\n",
        "\n",
        "    return masks\n",
        "\n",
        "@torch.no_grad()\n",
        "def medsam_inference(medsam_model, img_embed, box_256, new_size, original_size):\n",
        "    box_torch = torch.as_tensor(box_256, dtype=torch.float, device=img_embed.device)\n",
        "    if len(box_torch.shape) == 2:\n",
        "        box_torch = box_torch[:, None, :] # (B, 1, 4)\n",
        "\n",
        "    sparse_embeddings, dense_embeddings = medsam_model.prompt_encoder(\n",
        "        points = None,\n",
        "        boxes = box_torch,\n",
        "        masks = None,\n",
        "    )\n",
        "    low_res_logits, _ = medsam_model.mask_decoder(\n",
        "        image_embeddings=img_embed, # (B, 256, 64, 64)\n",
        "        image_pe=medsam_model.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n",
        "        sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n",
        "        dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n",
        "        multimask_output=False\n",
        "    )\n",
        "\n",
        "    low_res_pred = medsam_model.postprocess_masks(low_res_logits, new_size, original_size)\n",
        "    low_res_pred = torch.sigmoid(low_res_pred)\n",
        "    low_res_pred = low_res_pred.squeeze().cpu().numpy()\n",
        "    medsam_seg = (low_res_pred > 0.5).astype(np.uint8)\n",
        "\n",
        "    return medsam_seg\n",
        "\n",
        "\n",
        "\n",
        "def resize_longest_side(image, target_length=256):\n",
        "    \"\"\"\n",
        "    Resize image to target_length while keeping the aspect ratio\n",
        "    Expects a numpy array with shape HxWxC in uint8 format.\n",
        "    \"\"\"\n",
        "    oldh, oldw = image.shape[0], image.shape[1]\n",
        "    scale = target_length * 1.0 / max(oldh, oldw)\n",
        "    newh, neww = oldh * scale, oldw * scale\n",
        "    neww, newh = int(neww + 0.5), int(newh + 0.5)\n",
        "    target_size = (neww, newh)\n",
        "\n",
        "    return cv2.resize(image, target_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "def pad_image(image, target_size=256):\n",
        "    \"\"\"\n",
        "    Pad image to target_size\n",
        "    Expects a numpy array with shape HxWxC in uint8 format.\n",
        "    \"\"\"\n",
        "    # Pad\n",
        "    h, w = image.shape[0], image.shape[1]\n",
        "    padh = target_size - h\n",
        "    padw = target_size - w\n",
        "    if len(image.shape) == 3: ## Pad image\n",
        "        image_padded = np.pad(image, ((0, padh), (0, padw), (0, 0)))\n",
        "    else: ## Pad gt mask\n",
        "        image_padded = np.pad(image, ((0, padh), (0, padw)))\n",
        "\n",
        "    return image_padded\n",
        "\n",
        "def load_image(img_np,medsam_model):\n",
        "\n",
        "\n",
        "    if len(img_np.shape) == 2:\n",
        "        img_3c = np.repeat(img_np[:, :, None], 3, axis=-1)\n",
        "    else:\n",
        "        img_3c = img_np\n",
        "    assert np.max(img_3c)<256, f'input data should be in range [0, 255], but got {np.unique(img_3c)}'\n",
        "    H, W, _ = img_3c.shape\n",
        "\n",
        "    #segs = np.zeros(img_3c.shape[:2], dtype=np.uint8)\n",
        "\n",
        "    segs = np.zeros(img_3c.shape[:2], dtype=np.uint8)\n",
        "\n",
        "    ## MedSAM Lite preprocessing\n",
        "    img_256 = resize_longest_side(img_3c, 256)\n",
        "    newh, neww = img_256.shape[:2]\n",
        "    img_256_norm = (img_256 - img_256.min()) / np.clip(\n",
        "        img_256.max() - img_256.min(), a_min=1e-8, a_max=None\n",
        "    )\n",
        "    ## preprocessing\n",
        "    #img_256 = resize_longest_side(img_3c, 256)\n",
        "    #newh, neww = img_256.shape[:2]\n",
        "    img_256_padded = pad_image(img_256_norm, 256)\n",
        "    img_256_tensor = torch.tensor(img_256_padded).float().permute(2, 0, 1).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image_embedding = medsam_model.image_encoder(img_256_tensor)\n",
        "\n",
        "    return img_3c,image_embedding,H, W,newh, neww\n",
        "\n",
        "\n",
        "\n",
        "def save_mask(self):\n",
        "    out_path = f\"{self.image_path.split('.')[0]}_mask.png\"\n",
        "    io.imsave(out_path, self.mask_c)"
      ],
      "metadata": {
        "id": "C5nO_FEsgL7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantizing the whole linear layers of the models form 32 to 8 bits."
      ],
      "metadata": {
        "id": "hzI04NhwR7xc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lite_medsam_checkpoint_path=\"/content/drive/MyDrive/challenge_medsam/MedSAM_fast/work_dir/LiteMedSAM/lite_medsam.pth\"\n",
        "medsam_model=set_up_model()\n",
        "print(\"Loading MedSAM model, a sec.\")\n",
        "tic = time.perf_counter()\n",
        "lite_medsam_checkpoint = torch.load(lite_medsam_checkpoint_path, map_location=device)\n",
        "medsam_model.load_state_dict(lite_medsam_checkpoint)\n",
        "medsam_model.to(device)\n",
        "medsam_model.eval\n",
        "\n",
        "print(f\"Done, took {time.perf_counter() - tic} to load the model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8a92aaf-b335-415a-f812-b21e1f5abecd",
        "id": "Lep42FWsSFTm"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading MedSAM model, a sec.\n",
            "Done, took 0.34889948300042306 to load the model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Size of the model before quantization')\n",
        "print_size_of_model(medsam_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFjyUBqmW6zu",
        "outputId": "6bdb4994-f92e-4c04-a7db-95053156b5b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of the model before quantization\n",
            "Size (KB): 39376.561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**size reduction by 65.79 % !**"
      ],
      "metadata": {
        "id": "P7KfZRHVDznX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.ao.quantization import quantize_dynamic\n",
        "\n",
        "# Assuming img_encoder is your model\n",
        "medsam_model.eval()  # Ensure the model is in evaluation mode\n",
        "\n",
        "# Perform dynamic quantization, focusing on supported layers\n",
        "quantized_medsam_model = quantize_dynamic(\n",
        "    model=medsam_model,\n",
        "    dtype=torch.qint8,\n",
        "    qconfig_spec={\n",
        "        torch.nn.Linear,  # Add Linear as it's commonly quantized\n",
        "        # Embedding layers are not explicitly included here due to their special requirements\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"Dynamic quantization complete.\")\n",
        "print('Size of the model after quantization')\n",
        "print_size_of_model(quantized_medsam_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHbhjSY6YSWw",
        "outputId": "af39e2f3-7619-493a-adf0-fcca92a2ce2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dynamic quantization complete.\n",
            "Size of the model after quantization\n",
            "Size (KB): 13469.215\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMiFkPZpUMQT+g3rL5713ic",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}